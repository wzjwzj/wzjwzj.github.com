---
layout: post
title: "ABIC"
category: "uncategorized"
tags: info
 
toc: true
widget:
mathjax: true
prettify: 
htmlhead: 
---
{% include JB/setup %}
 
##summary

+ the soultion was independent of the initial values of the model parameters.
+ more stronger smoothing constraint. more plausible slip distribution can be obtained, but then the moment-rate function becomes even smoother.
+ high-frequency components were well reproduced only by the new formulation.


<script type="math/tex; mode=display">
min: \left(d-Ha\right)^t E^{-1} (d-Ha)
</script>

<!--end_excerpt-->

## mathematical formulation

**forward modeling:**

<script type="math/tex; mode=display">
u_j\left(t\right)=\sum_{q=1}^2 \int_s G_{qj}^0\left(t,\xi\right)*\dot{D}_q^0\left(t,\xi\right)~d\xi +e_{bj}\left( t \right)
</script>

+ *j*: station index. 
+ *q*: slip component, along **strike** and **dip** direction.
+ integration in space :  $ \int_s ~d\xi $ 
+ integration in time :  convolution operator \* 

** suppose: **
<script type="math/tex; mode=display">
\dot{D}_q^0\left(t,\xi\right) \thickapprox \sum_{k=1}^K \sum_{l=1}^L a_{qkl}X_k\left(\xi\right)T_l\left(t-t_k\right)
</script>

+ $ X_k $ : basis functions for space. 
+ $ T_l $ : basis functions for time    
+ $ t_k $ : arrival time of rupture front at grid k. 
+ Note : we just convert to estimating $ a_{qkl} $ 
         there is no need to be orthogonal basis, here adopt Bézier basis functions. 
         so the shape can be control by neibourhood grid which favor applying smoothing constrians.

---

<script type="math/tex; mode=display">
\begin{aligned}

u_j\left(t\right)&=\sum_{q=1}^2 \int_s G_{qj}^0\left(t,\xi\right)*  \color{red}{ \dot{D}_q^0\left(t,\xi\right) } ~d\xi +e_{bj}\left( t \right)\\
                 &=\sum_{q=1}^2 \int_s G_{qj}^0\left(t,\xi\right)*  \color{red}{ \sum_{k=1}^K \sum_{l=1}^L a_{qkl}X_k\left(\xi\right)T_l\left(t-t_k\right)} ~d\xi +e_{bj}\left( t \right)\\
                 &=\sum_{q=1}^2  \sum_{k=1}^K \sum_{l=1}^L a_{qkl} T_l\left(t-t_k\right) * \color{red}{ \int_s  X_k\left(\xi\right)G_{qj}^0\left(t,\xi\right) ~d\xi} +e_{bj}\left( t \right)\\
                 &=\sum_{q=1}^2 \sum_{k=1}^K \sum_{l=1}^L a_{qkl} T_l \left( t - t_k \right) * \color{red}{ g_{qkj}^0\left( t \right)} +e_{bj}\left( t \right) \\

\end{aligned}
</script>

with 
<script type="math/tex; mode=display">
 g_{qkj}^0\left( t \right) = \int_s X_k \left( \xi \right) G_{qj}^0\left(t,\xi\right) ~d\xi
</script>

suppose: 
<script type="math/tex; mode=display">
g_{qkj}^0\left( t \right) = g_{qkj}  \left( t \right) + \delta g_{qkj} \left( t \right) 
</script>

<script type="math/tex; mode=display">
\begin{aligned}
u_j \left( t \right) & = \sum_{q=1}^2 \sum_{k=1}^K 
                       \left[  
                          \sum_{l=1}^L \bbox[#eee]{ T_l\left(t-t_k\right)*g_{qkj}\left( t \right) } \color{red}{ a_{qkl}}    + 
                          \bbox[#eee]{  \sum_{l=1}^L a_{qkl}T_l\left(t-t_k\right)}  * \color{red}{ \delta g_{qkj} \left( t \right) }
                       \right] 
                     + e_{bj}\left( t \right) \\
		& = \sum_{q=1}^2 \sum_{k=1}^K 
                       \left[  
                          \sum_{l=1}^L \bbox[#eee]{ \widetilde{H}_{qklj}\left(t\right) }  \color{red}{ a_{qkl}}    + 
                          \bbox[#eee]{  \widetilde{P}_{qk}\left(t,a_{qkl}\right)} * \color{red}{ \delta g_{qkj} \left( t \right) }
                       \right] 
                     + e_{bj}\left( t \right)
\end{aligned}
</script>

**apply filter $B(t)$ **

<script type="math/tex; mode=display">
\begin{aligned}
d_j \left( t \right) & = B\left(t\right) * u_j\left( t\right) \\
 		     & = \sum_{q=1}^2 \sum_{k=1}^K 
                       \left[  
                          \sum_{l=1}^L \bbox[#eee]{ B\left(t\right)*T_l\left(t-t_k\right)*g_{qkj}\left( t \right) } \color{red}{ a_{qkl}}    + 
                          \bbox[#eee]{  \sum_{l=1}^L B\left(t\right)*a_{qkl}T_l\left(t-t_k\right)}  * \color{red}{ \delta g_{qkj} \left( t \right) }
                       \right] 
                     + B\left(t\right)*e_{bj}\left( t \right) \\
		     & = \sum_{q=1}^2 \sum_{k=1}^K 
                       \left[  
                          \sum_{l=1}^L \bbox[#eee]{ B\left(t\right)*\widetilde{H}_{qklj}\left(t\right) }  \color{red}{ a_{qkl}}    + 
                          \bbox[#eee]{  B\left(t\right)*\widetilde{P}_{qk}\left(t,a_{qkl}\right)} * \color{red}{ \delta g_{qkj} \left( t \right) }
                       \right] 
                     + B\left(t\right)*e_{bj}\left( t \right) \\
		     & = \sum_{q=1}^2 \sum_{k=1}^K 
                       \left[  
                          \sum_{l=1}^L \bbox[#eee]{ H_{qklj}\left(t\right) }  \color{red}{ a_{qkl}}    + 
                          \bbox[#eee]{  P_{qk}\left(t,a_{qkl}\right)} * \color{red}{ \delta g_{qkj} \left( t \right) }
                       \right] 
                     + B\left(t\right)*e_{bj}\left( t \right)
		
\end{aligned}
</script>

---
<script type="math/tex; mode=display">
\begin{aligned}
d_j &= H_ja+e_j\left(a\right) \\
with &: \\
e_j(a) &= \sum_{q=1}^2 \sum_{k=1}^K P_{qkj}\left(a\right)  \delta g_{qkj} \left( t \right) + B_je_{bj}
\end{aligned}
</script>
---
<script type="math/tex; mode=display">
\begin{aligned}
\widetilde{e}_j(t;a) & = \sum_{q=1}^2 \sum_{k=1}^K 
           \bbox[#eee]{  P_{qk}\left(t,a_{qkl}\right)} * \color{red}{ \delta g_{qkj} \left( t \right) }
           + B\left(t\right)*e_{bj}\left( t \right) \\
	  & = \sum_{q=1}^2 \sum_{k=1}^K 
            \bbox[#eee]{  B\left(t\right)*\widetilde{P}_{qk}\left(t,a_{qkl}\right)} * \color{red}{ \delta g_{qkj} \left( t \right) }
          + B\left(t\right)*e_{bj}\left( t \right) \\
          &=\bbox[#eee]{ \sum_{q=1}^2 \sum_{k=1}^K 
            \bbox[#fee]{  \sum_{l=1}^L B\left(t\right)*a_{qkl}T_l\left(t-t_k\right)}  * \color{red}{ \delta g_{qkj} \left( t \right) }
           + B\left(t\right)*e_{bj}\left( t \right)} \\
	  &=\bbox[#eee]{\sum_{q=1}^2 \sum_{k=1}^K 
           \bbox[#fee]{  \sum_{l=1}^L B\left(t\right)*a_{qkl}T_l\left(t-t_k\right)}  * \color{red}{ \int_s X_k \left( \xi \right)   \delta G_{qj}\left(t,\xi\right) ~d\xi }
           + B\left(t\right)*e_{bj}\left( t \right)} \\
	  &=B\left(t\right)  * \sum_{q=1}^2 \color{red}{ \int_s \sum_{k=1}^K 
            \sum_{l=1}^L   a_{qkl} X_k \left( \xi \right) T_l\left(t-t_k\right) * \delta G_{qj}\left(t,\xi\right) ~d\xi }
           + B\left(t\right)*e_{bj}\left( t \right) \\
	  &=B\left(t\right)  * \sum_{q=1}^2 \color{red}{ \int_s \dot{D}_q^0\left(t,\xi\right) * \delta G_{qj}\left(t,\xi\right) ~d\xi }
           + B\left(t\right)*e_{bj}\left( t \right) \\
\end{aligned}
</script>

---

## Propagation of uncertainty 

**[variance-covariance matrix](http://en.wikipedia.org/wiki/Variance-covariance_matrix) **

<script type="math/tex; mode=display">
\begin{aligned}

\Sigma_{ij} &= \mathrm{cov}(X_i, X_j) = \mathrm{E}\begin{bmatrix}
(X_i - \mu_i)(X_j - \mu_j)\\
\end{bmatrix} \\

&= \begin{bmatrix}
 \mathrm{E}[(X_1 - \mu_1)(X_1 - \mu_1)] & \mathrm{E}[(X_1 - \mu_1)(X_2 - \mu_2)] & \cdots & \mathrm{E}[(X_1 - \mu_1)(X_n - \mu_n)] \\ \\
 \mathrm{E}[(X_2 - \mu_2)(X_1 - \mu_1)] & \mathrm{E}[(X_2 - \mu_2)(X_2 - \mu_2)] & \cdots & \mathrm{E}[(X_2 - \mu_2)(X_n - \mu_n)] \\ \\
 \vdots & \vdots & \ddots & \vdots \\ \\
 \mathrm{E}[(X_n - \mu_n)(X_1 - \mu_1)] & \mathrm{E}[(X_n - \mu_n)(X_2 - \mu_2)] & \cdots & \mathrm{E}[(X_n - \mu_n)(X_n - \mu_n)]
\end{bmatrix} \\

&= \begin{pmatrix}
   \sigma^2_1 & \text{cov}_{12} & \text{cov}_{13} & \cdots \\
   \text{cov}_{12} & \sigma^2_2 & \text{cov}_{23} & \cdots\\
   \text{cov}_{13} & \text{cov}_{23} & \sigma^2_3 & \cdots \\
   \vdots & \vdots & \vdots & \ddots \\
\end{pmatrix} \\


\end{aligned}
</script>

<script type="math/tex; mode=display">
\begin{aligned}
f_k &= \sum_i^n A_{ki} x_i  , (k=1\dots m); \mathbf{f} =\mathbf{A_{mn}x}  \\
\Sigma^f_{ij}&= \sum_k^n \sum_\ell^n A_{ik} \Sigma^x_{k\ell} A_{j\ell} \\
\Sigma^f&=\mathbf{A} \Sigma^x \mathbf{A}^\top \\
\Sigma^f_{ij}&= \sum_k^n  A_{ik} \left(\sigma^2_k \right)^x A_{jk}
\end{aligned}
</script>



**[Discrete convolution](http://en.wikipedia.org/wiki/Toeplitz_matrix)**

<script type="math/tex; mode=display">

        y = h \ast x =
            \begin{bmatrix}
                h_1 & 0 & \ldots & 0 & 0 \\
                h_2 & h_1 & \ldots & \vdots & \vdots \\
                h_3 & h_2 & \ldots & 0 & 0 \\
                \vdots & h_3 & \ldots & h_1 & 0 \\
                h_{m-1} & \vdots & \ldots & h_2 & h_1 \\
                h_m & h_{m-1} & \vdots & \vdots & h_2 \\
                0 & h_m & \ldots & h_{m-2} & \vdots \\
                0 & 0 & \ldots & h_{m-1} & h_{m-2} \\
                \vdots & \vdots & \vdots & h_m & h_{m-1} \\
                0 & 0 & 0 & \ldots & h_m
            \end{bmatrix}
            \begin{bmatrix}
                x_1 \\
                x_2 \\
                x_3 \\
                \vdots \\
                x_n
            \end{bmatrix}
</script>

## [Bézier curve](http://en.wikipedia.org/wiki/B%C3%A9zier_curve)
### recursion formula
<script type="math/tex; mode=display">
\begin{aligned}
B_{j,0}(x) &:= \left\{
\begin{matrix} 
1 & \mathrm{if} \quad t_j \leq x < t_{j+1} \\
0 & \mathrm{otherwise} 
\end{matrix}\right. \\
B_{i,k}(x) &:= \frac{x - t_i}{t_{i+k-1} - t_i} B_{i,k-1}(x) + \frac{t_{i+k} - x}{t_{i+k} - t_{i+1}} B_{i+1,k-1}(x).
\end{aligned}
</script>

### General form of a NURBS surface
A NURBS surface is obtained as the tensor product of two NURBS curves, thus using two independent parameters u and v (with indices i and j respectively):
<script type="math/tex; mode=display">
S(u,v) = \sum_{i=1}^k \sum_{j=1}^l R_{i,j}(u,v) \mathbf{P}_{i,j} 
</script>
with
<script type="math/tex; mode=display">
R_{i,j}(u,v) = \frac {N_{i,n}(u) N_{j,m}(v) w_{i,j}} {\sum_{p=1}^k \sum_{q=1}^l N_{p,n}(u) N_{q,m}(v) w_{p,q}}
</script>
as rational basis functions.



## simple introduction of Bayesian rule
Joint probability of two events A and B:
<script type="math/tex; mode=display">
P\left(AB\right) = P\left(A|B\right)P\left(B\right)=P\left(B|A\right)P\left(A\right)
</script>

In Bayesian probablity theory: one "events" is <font color=red>H</font>ypothesis. the other is <font color=red>D</font>ata. so:
<script type="math/tex; mode=display">
P\left(H|D\right) = \frac{P\left(D|H\right)P\left(H\right)}{P\left(D\right)}
</script>
+ $P\left(D|H\right)$: likelihood function, as it assesses the probablity of the observed data arising from hypothesis. 
+ $P\left(H\right)$: prior, as it reflects one&prime; prior knowledge before the data are considered.
+ $P\left(H|D\right)$: Posterior, as its name suggests. reflects the probability of the hypothesis after consideration.

**A simple example**

let&prime;s say we have some quantity in the world, $x$, and our observation of this quantity,$y$, is corrupted by additive Gaussian noise, e:  
<script type="math/tex; mode=display">
y=x+e, \quad e ~ \left(0, \sigma^2\right)
</script>
+ we might want to pick the value of $x$ that maximizes this distribution 
<script type="math/tex; mode=display">
\widehat{x} = arg\,\underset{x}{min}\,P\left(x|y\right)
</script>  

+ alternatively, we may want minimize the mean squared error of our guesses, then we should pick the mean of (Px|y);  
<script type="math/tex; mode=display">
\widehat{x} = \int x P\left(x|y\right)~dx
</script>  

let&prime;s draw upon our existing knowledge. x with mean of 12,variance of 1. Thus,

<script type="math/tex; mode=display">
\begin{aligned}
P\left(x|y\right) &= \frac{P\left(y|x\right)P\left(x\right)}{P\left(y\right)} \\
P\left(y|x\right) &= \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\left(y-x\right)^2}{2\sigma^2}} \\
P\left(x\right)   &= \frac{1}{2\pi}e^{-\frac{\left(x-12\right)^2}{2}}\\
P\left(x|y\right) &\varpropto  e^{-\frac{\left(y-x\right)^2}{2\sigma^2}}e^{-\frac{\left(x-12\right)^2}{2}}
\end{aligned}
</script>

The x which maximizes $P(x|y)$ is the same as that which minimizes the exponent in
brackets which may be found by simple algebraic manipulation to be:

<script type="math/tex; mode=display">
\widehat{x}=\frac{\mathbf{y}+12\sigma^2}{1+\sigma^2}
</script>

----
## normalized bicubic B-splines
<script type="math/tex; mode=display">
\Phi_{kl}\left(\xi_1,\xi_2\right)=N_k\left(\xi_1\right)N_l\left(\xi_2\right)\qquad \left(k=1,...,K;l=1,...,L\right)
</script>
where
<script type="math/tex; mode=display">
N_j\left(s\right)=4 \Delta s M_{4,j+2}\left(s\right)
</script>

which $ M_{4,j}(s) $ is the B-spline function of order 4(degree 3)

<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
M_{1,j}\left(s\right) &=
  \begin{cases}
    1/\Delta s & \left( s_j - \Delta s  \leq  s               \lt    s_j \right) \\
    0         & \left( s               \lt   s_j - \Delta s ; s_j \leq   s   \right) 
  \end{cases} \\  

M_{r,j}\left( s \right) &= \frac{1}{ r \Delta s} \left\{ \cancelto{ \left( s-s_{j-r} \right) }{ \left[   s-s_j+\left( j-r \right) \Delta s  \right] } M_{r-1,j-1}\left( s \right) + \left( s_j - s \right) M_{r-1,j}\left( s \right) \right\}
\end{aligned}
\end{equation}
</script>


<script type="math/tex; mode=display">
\begin{aligned}
M_{1,j}\left(s\right) &=
  \begin{cases}
    1/\Delta s & \left( s_j - \Delta s  \leq  s               \lt    s_j \right) \\
    0         & \left( s               \lt   s_j - \Delta s ; s_j \leq   s   \right) 
  \end{cases} \\  
M_{r,j}\left( s \right) &= \frac{1}{ r \Delta s} \left\{ \left( s-s_{j-r}  \right) M_{r-1,j-1}\left( s \right) + \left( s_j - s \right) M_{r-1,j}\left( s \right) \right\} 
\end{aligned}
</script>

<script type="math/tex; mode=display">
\begin{aligned}
\Delta s M_{1,j}\left(s\right) &=
  \begin{cases}
    1          & \left( s_{j-1}  \leq  s    \lt    s_j \right) \\
    0          & \left( s        \lt   s_{j-1} ;   s_j  \leq   s   \right) 
  \end{cases} \\  
2 \Delta s^2 M_{2,j}\left( s \right) &= 
  \begin{cases}
    \left( s-s_{j-2}\right) & \left( s_{j-2}  \leq  s   \lt    s_{j-1} \right) \\
    \left( s_j-s   \right)  & \left( s_{j-1}  \leq  s   \lt    s_j \right) \\
    0          & \left( s        \lt   s_{j-2} ;  s_j \leq   s   \right) 
  \end{cases} \\  
6 \Delta s^3 M_{3,j}\left( s \right) &= 
  \begin{cases}
    \left( s-s_{j-3}\right)^2                             & \left( s_{j-3}  \leq  s   \lt    s_{j-2}   \right) \frac{ 1}{3}\Delta s^3 \\
    \left( s-s_{j-3}\right)\left(s_{j-1}-s\right)
   +\left( s_j-s \right)\left(s-s_{j-2}\right)            & \left( s_{j-2}  \leq  s   \lt    s_{j-1}   \right) \frac{ 4}{3}\Delta s^3 \\
    \left( s_j-s   \right)^2                              & \left( s_{j-1}  \leq  s   \lt    s_j \quad \right) \frac{ 1}{3}\Delta s^3 \\
    0          & \left( s        \lt   s_{j-3} ;  s_j \leq   s   \right) 
  \end{cases} \\  
24 \Delta s^4 M_{4,j}\left( s \right) &= 
  \begin{cases}
    \left( s-s_{j-4}\right)^3                                     & \left( s_{j-4}  \leq  s   \lt    s_{j-3} \right) \\
    \left( s - s_{j-4}\right)^2 \left(s_{j-2} - s\right) +
    \left( s - s_{j-4}\right)   \left(s_{j-1} - s\right)    \left(s - s_{j-3}\right) +
    \left( s_j-s \right)        \left(s-s_{j-3}\right)^2          & \left( s_{j-3}  \leq  s   \lt    s_{j-2} \right) \\
    \left( s-s_{j-4}\right)     \left(s_{j-1}-s\right)^2 +
    \left( s_j - s \right)      \left(s - s_{j-3}\right)    \left(s_{j-1} - s\right) + 
    \left( s_j - s \right)^2    \left(s - s_{j-2}\right)          & \left( s_{j-2}  \leq  s   \lt    s_{j-1} \right) \\
    \left( s_j - s \right)^3  & \left( s_{j-1}  \leq  s   \lt    s_j \right) \\
    0          & \left( s        \lt   s_{j-4} ;  s_j \leq   s   \right) 
  \end{cases} \\  
 &= 
  \begin{cases}
    \left( 2\Delta s - \left( s_{j-2}-s \right)\right)^3                                 & \left( s_{j-4} \leq s \lt s_{j-3} \right) \frac{ 1}{4}\Delta s^4 \\
    3 \left( s_{j-2}-s \right)^3 -6 \Delta s \left( s_{j-2}-s \right)^2  + 4 \Delta s^3  & \left( s_{j-3} \leq s \lt s_{j-2} \right) \frac{11}{4}\Delta s^4 \\
    3 \left( s-s_{j-2} \right)^3 -6 \Delta s \left( s-s_{j-2} \right)^2  + 4 \Delta s^3  & \left( s_{j-2} \leq s \lt s_{j-1} \right) \frac{11}{4}\Delta s^4 \\
    \left( 2\Delta s - \left( s-s_{j-2}\right)\right)^3                                  & \left( s_{j-1} \leq s \lt s_j\quad \right) \frac{ 1}{4}\Delta s^4 \\
    0          & \left( s        \lt   s_{j-4} ;  s_j \leq   s   \right) 
  \end{cases} \\  
\end{aligned}
</script>
<br/><br/>
<script type="math/tex; mode=display">
\begin{aligned}
24 \Delta s^4 M_{4,j+2}\left( s \right) &= 
  \begin{cases}
    \left( s-s_{j-2}\right)^3                                     & \left( s_{j-2}  \leq  s   \lt    s_{j-1} \right) \\
    \left( s - s_{j-2}\right)^2 \left(s_j - s\right) +
    \left( s - s_{j-2}\right)   \left(s_{j+1} - s\right)    \left(s - s_{j-1}\right) +
    \left( s_{j+2}-s \right)        \left(s-s_{j-1}\right)^2          & \left( s_{j-1}  \leq  s   \lt    s_j \right) \\
    \left( s-s_{j-2}\right)     \left(s_{j+1}-s\right)^2 +
    \left( s_{j+2} - s \right)      \left(s - s_{j-1}\right)    \left(s_{j+1} - s\right) + 
    \left( s_{j+2} - s \right)^2    \left(s - s_j\right)          & \left( s_j  \leq  s   \lt    s_{j+1} \right) \\
    \left( s_{j+2} - s \right)^3  & \left( s_{j+1}  \leq  s   \lt    s_{j+2} \right) \\
    0          & \left( s        \lt   s_{j-2} ;  s_{j+2} \leq   s   \right) 
  \end{cases} \\  
 &= 
  \begin{cases}
    \left( s-s_{j-2}\right)^3   = \left( 2\Delta s - \left( s_j-s \right)\right)^3    & \left( s_{j-2} \leq s \lt s_{j-1} \right) \frac{ 1}{4}\Delta s^4  \\
    3 \left( s_j-s \right)^3 -6 \Delta s \left( s_j-s \right)^2  + 4 \Delta s^3      & \left( s_{j-1} \leq s \lt s_j \quad \right) \frac{11}{4}\Delta s^4 \\
    3 \left( s-s_j \right)^3 -6 \Delta s \left( s-s_j \right)^2  + 4 \Delta s^3      & \left( s_j \leq s \lt s_{j+1} \quad \right) \frac{11}{4}\Delta s^4 \\
    \left( s_{j+2} - s \right)^3 = \left( 2\Delta s - \left( s-s_j\right)\right)^3   & \left( s_{j+1} \leq s \lt s_{j+2}\quad \right) \frac{ 1}{4}\Delta s^4  \\
    0          & \left( s        \lt   s_{j-2} ;  s_{j+2} \leq   s   \right) 
  \end{cases} \\  

24 \Delta s^4 \frac{  \partial M_{4,j+2}\left( s \right) }{\partial s} &= 
  \begin{cases}
   \enspace \,   3(s-s_{j-2})^2                        & \left( s_{j-2}  \leq  s   \lt    s_{j-1} \right) \\
      -9(s_j-s)^2+12\Delta s (s_j-s)         & \left( s_{j-1}  \leq  s   \lt    s_j \right) \\
   \enspace \,   9(s-s_j)^2-12\Delta s (s-s_j)         & \left( s_j  \leq  s   \lt    s_{j+1} \right) \\
      -3(s_{j+2}-s)^2                        & \left( s_{j+1}  \leq  s   \lt    s_{j+2} \right) \\
   \enspace \,  0                                     & \left( s        \lt   s_{j-2} ;  s_{j+2} \leq   s   \right) 
  \end{cases} \\  
24 \Delta s^4 \frac{  \partial^2 M_{4,j+2}\left( s \right) }{\partial s^2} &= 
  \begin{cases}
  \enspace 6(s-s_{j-2})    & \left( s_{j-2} \leq s \lt s_{j-1} \right)  \\
    18(s_j-s)-12 \Delta s  & \left( s_{j-1} \leq s \lt s_j \quad \right)  \\
    18(s-s_j)-12 \Delta s  & \left( s_j \leq s \lt s_{j+1} \quad \right)  \\
  \enspace 6(s_{j+2}-s)    & \left( s_{j+1} \leq s \lt s_{j+2}\quad \right)  \\
  \enspace 0               & \left( s  \lt   s_{j-2} ;  s_{j+2} \leq   s   \right) 
  \end{cases} \\

\end{aligned}
</script>

<script type="math/tex; mode=display">
\begin{aligned}
\int M_{n,j}(s)~ds=\frac{1}{n}  
\end{aligned}
</script>

<figure class="one">
	<img style="border: none;" src="/images/figures/2014030301.png">
	<figcaption><span style="color:red">Red:M4;</span><span style="color:green">Green: first derivative;</span><span style="color:blue">Blue: second derivative</span></figcaption>
</figure>

**value of center point**
<script type="math/tex; mode=display">
\begin{aligned}
M^c_{1,j}\left( s_j-0.5\Delta s \right) &= 1/\Delta s\\
M^c_{2,j}\left( s_j-1.0\Delta s \right) &= 1/2\Delta s\\
M^c_{3,j}\left( s_j-1.5\Delta s \right) &= 1/4\Delta s\\
M^c_{4,j}\left( s_j-2.0\Delta s \right) &= 1/6\Delta s\\
\end{aligned}
</script>

<script type="math/tex; mode=display">
\begin{matrix}
      &       &       &       &       & 1      & 0     & \quad\frac{1}{1!\Delta s} & M_{1,j}     \\
      &       &       &       & 0     & 1      & 0     & \quad\frac{1}{2!\Delta s} & M_{2,j}     \\
      &       &       & 0     & 1     & 1      & 0     & \quad\frac{1}{3!\Delta s} & M_{3,j}     \\
      &       &   0   & 1     & 4     & 1      & 0     & \quad\frac{1}{4!\Delta s} & M_{4,j}     \\
      &   0   &   1   & 11    & 11    & 1      & 0     & \quad\frac{1}{5!\Delta s} & M_{5,j}     \\
    0 &   1   &   26  & 66    & 26    & 1      & 0     & \quad\frac{1}{6!\Delta s} & M_{6,j}     \\
s_{j-6} & s_{j-5} & s_{j-4} & s_{j-3} & s_{j-2} & s_{j-1}  & s_{j}   &             &             \\
\end{matrix}
</script>

**value of grid**
<script type="math/tex; mode=display">
\begin{matrix}
    &     &      &      &      & 1_1  & 0_0  & \quad\frac{1}{1!\Delta s} & M_{1,j}     \\
    &     &      &      & 1_0  & 0_1  &      &              &                                    \\
    &     &      &      & 0_2  & 1_1  & 0_0  & \quad\frac{1}{2!\Delta s} & M_{2,j}     \\
    &     &      & 0_0  & 1_1  & 0_2  &      &              &                                    \\
    &     &      & 0_2  & 1_2  & 1_1  & 0_0  & \quad\frac{1}{3!\Delta s} & M_{3,j}     \\
    &     & 0_0  & 1_1  & 1_2  & 0_3  &      &              &                                    \\
    &     & 0_4  & 1_3  & 4_2  & 1_1  & 0_0  & \quad\frac{1}{4!\Delta s} & M_{4,j}     \\
    & 0_0 & 1_1  & 4_2  & 1_3  & 0_4  &      &              &                                    \\
    & 0_5 & 1_4  & 11_3 & 11_2 & 1_1  & 0_0  & \quad\frac{1}{5!\Delta s} & M_{5,j}     \\
0_0 & 1_1 & 11_2 & 11_3 & 1_4  & 0_5  &      &              &                                    \\
0_6 & 1_5 & 26_4 & 66_3 & 26_2 & 1_1  & 0_0  & \quad\frac{1}{6!\Delta s} & M_{6,j}     \\
s_{j-6} & s_{j-5} & s_{j-4} & s_{j-3} & s_{j-2} & s_{j-1}  & s_{j}  &              &             \\
\end{matrix}
</script>

**integration of $M_{r,j}$ **
<script type="math/tex; mode=display">
\int M_{r,j} = \frac {1}{r}
</script>

## roughness
<script type="math/tex; mode=display">
\begin{aligned}
& \left(\sum_{i=1}^n a_i\right)^2 = \sum_{k,l=1}^n a_k a_l \qquad \left(\sum_{i,j=1}^n a_{ij}\right)^2 = \sum_{k,l,p,q=1}^n a_{kl} a_{pq} \\
\Delta u_j&\left(\xi_1,\xi_2\right)=\sum^K_{k=1}\sum^K_{l=1} a_{jkl}\Phi_{kl}\left(\xi_1,\xi_2\right)=\sum^K_{k=1}\sum^K_{l=1} a_{jkl}N_k\left(\xi_1\right)N_l\left(\xi_2\right)\\
r&=\sum^2_{j=1}\int_s \frac{1}{n_3}\left[\left(\frac{1}{h^2_1}\frac{\partial^2\Delta u_j}{\partial \xi^2_1}\right)^2+2\left(\frac{1}{h_1 h_2}\frac{\partial^2\Delta u_j}{\partial\xi_1\partial\xi_2}\right)^2+\left(\frac{1}{h^2_2}\frac{\partial^2\Delta u_j}{\partial \xi^2_2}\right)^2\right]~d\xi_1d\xi_2 \\
&=\sum^2_{j=1}\sum^K_{k=1}\sum^L_{l=1}\sum^K_{p=1}\sum^L_{q=1}a_{jkl}R_{jklpq}a_{jpq} \\
R_{jklpq}&=\frac{1}{\overline{n}_3 \overline{h}^4_1}\int\frac{\partial^2N_k(s)}{\partial s^2}\frac{\partial^2N_p(s)}{\partial s^2}~ds\int N_l(s)N_q(s)~ds \\
&+ \frac{2}{\overline{n}_3 \overline{h}^2_1 \overline{h}^2_2}\int\frac{\partial N_k(s)}{\partial s}\frac{\partial N_p(s)}{\partial s}~ds\int\frac{\partial N_l(s)}{\partial s}\frac{\partial N_q(s)}{\partial s}~ds \\
&+\frac{1}{\overline{n}_3 \overline{h}^4_2}\int N_k(s)N_p(s)~ds\int\frac{\partial^2N_l(s)}{\partial s^2}\frac{\partial^2N_q(s)}{\partial s^2}~ds \\
&=\widetilde{R}_{klpq} \\
\text{with}  \\
h_i &= \sqrt{1+f_i}\qquad\left(i=1,2\right) \\
\text{in} & \text{ vector form }  \\
r&=\mathbf{a}^T\mathbf{G}\mathbf{a}
\text{ with } & \\
G_{IJ} &= R{jklpq} \\
I&=(j-1)KL+(k-1)L+l \\
J&=(j-1)KL+(p-1)L+q
\end{aligned} 
</script>


suppose coordinate on fault then $f_1=f_2=0, h_1=h_2=1$


M_4,i 积分，微分，最大值，
M_4,j * M_4,j 积分微分

<script type="math/tex; mode=display">
\begin{aligned}
&\left(s-s_{j-3}\right)\left(s_{j-1}-s\right)+\left(s_j-s\right)\left(s-s_{j-2}\right) \\
&=\left( \Delta s + \left( s- s_{j-2} \right) \right)\left( \Delta s - \left( s- s_{j-2} \right) \right) 
 +\left( \Delta s + \left( s_{j-1}- s \right) \right)\left( \Delta s - \left( s_{j-1}- s \right) \right)\\
&=\color{red}{\left( {\Delta s}^2 -\left( s-s_{j-2}\right)^2\right) 
 +\left( {\Delta s}^2 -\left( s_{j-1}-s\right)^2\right)} \\
&=\frac{3}{2}\Delta s^2 - 2\left(s-s_{j-1.5}\right)^2
\end{aligned}
</script>



---

**suppose:**
errors $e$ to be Gaussian with 

<script type="math/tex; mode=display">
e \sim N(0,\sigma^2 \mathbf{E}) 
</script>

$\sigma ^2 $ is unknow scale factor

**likelihood function:**
<script type="math/tex; mode=display">
p\left( \mathbf{d} \mid \mathbf{a} \, ; \sigma^2 \right) = \left( 2 \pi \sigma^2 \right)^{-N/2} \| E \| ^{-1/2}  \times exp \left[ - \frac{1}{2\sigma^2} \left( \mathbf{d-Ha}\right)^T \mathbf{E}^{-1} \left( \mathbf{d-Ha} \right) \right]
</script>

here $ || E || $ denotes the absolute value of the determinant of $ \mathbf{E} $

**prior information:**
<script type="math/tex; mode=display">
p\left( \mathbf{a} \, ; \rho^2 \right) = \left( 2 \pi \rho^2 \right)^{-P/2} \| \Lambda_p \| ^{-1/2}   exp \left( - \frac{1}{2\rho^2} \mathbf{a}^T\mathbf{G} \mathbf{a} \right) 
</script>

**posterior pdf**
<script type="math/tex; mode=display">
\begin{equation}
P(\mathbf{a}\,;\,\sigma^2,\,\rho^2\,\mid\,\mathbf{d}) \propto P(\mathbf{d}\,\mid\,\mathbf{a}\,;\,\sigma^2)P(\mathbf{a}\,;\,\rho^2) 
\end{equation}
</script>
**Marginal likelihood**
<script type="math/tex; mode=display">
\begin{equation}
L(\sigma^2,\rho^2) = \int P(\mathbf{d}\,\mid\,\mathbf{a}\,;\,\sigma^2)P(\mathbf{a}\,;\,\rho^2) ~d\mathbf{a}
\end{equation}
</script>
**Akaike Bayesian Information Criterion(ABIC)**
<script type="math/tex; mode=display">
ABIC=(-2) log L(\sigma^2,\rho^2)
</script>

---

<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
P(\mathbf{a};\sigma^2,\alpha^2 \mid \mathbf{d}) & = c(2\pi\sigma^2)^{-(N+P)/2}(\alpha^2)^{p/2}\|\mathbf{E}\|^{-1/2}\|\Lambda_p\|^{1/2} \\ 
                                                & \times exp\left[ -\frac{1}{2\sigma^2} s(\mathbf{a})\right] 
\end{aligned}
\end{equation}
</script>

with 
<script type="math/tex; mode=display">
\begin{equation}
s(\mathbf{a}) = (\mathbf{d-Ha})^T\mathbf{E}^{-1}(\mathbf{d-Ha})+\alpha^2\mathbf{a^TGa}
\end{equation}
</script>

for certain values of $\sigma ^2 $ and $\alpha ^2 $, minimizing $s(\mathbf{a}) $, $\frac{\partial s(\mathbf{a})}{\partial \mathbf{a}}=0 $

<script type="math/tex; mode=display">
\begin{equation}
\mathbf{H^TE^{-1}}(\mathbf{d-Ha^\star})-\alpha^2\mathbf{Ga^\star=0}
\end{equation}
</script>

the solution is:

<script type="math/tex; mode=display">
\begin{equation}
\mathbf{a^\star=(H^TE^{-1}H}+\alpha^2\mathbf{G)^{-1}H^TE^{-1}d}
\end{equation}
</script>

then, denoting

<script type="math/tex; mode=display">
\begin{equation}
s(\mathbf{a^\star}) = (\mathbf{d-Ha^\star})^T\mathbf{E}^{-1}(\mathbf{d-Ha^\star})+\alpha^2 \mathbf{a^{\star T}Ga^\star}
\end{equation}
</script>

rewrite $s(\mathbf{a}) $

<script type="math/tex; mode=display">
\begin{equation}
s(\mathbf{a})=s(\mathbf{a^\star}) + (\mathbf{a-a^\star})^T(\mathbf{H^TE^{-1}H}+\alpha^2\mathbf{G})(\mathbf{a-a^\star})
\end{equation}
</script>


**[Gaussian integral](http://en.wikipedia.org/wiki/Gaussian_integral)**
Suppose **A** is a symmetric positive-definite (hence invertible) $n \times n$ covariance matrix. Then,

<script type="math/tex; mode=display">
\begin{equation}
\int_{-\infty}^\infty \exp\left(-\frac 1 2 x^{T} A x \right) \, d^nx=\sqrt{\frac{(2\pi)^n}{\det A}} 
\end{equation}
</script>

subsitute equation (9) into equation (4), carrying out the integration of marginal likelihood equation (3) (see gaussian integral eq.(10)).
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
L&(\sigma^2,\rho^2)  = \int P(\mathbf{d}\,\mid\,\mathbf{a}\,;\,\sigma^2)P(\mathbf{a}\,;\,\rho^2) ~d\mathbf{a} \\
                   & = (2\pi\sigma^2)^{-(N+P)/2}(\alpha^2)^{p/2}\|\mathbf{E}\|^{-1/2}\|\Lambda_p\|^{1/2} \times 
                       exp\left[ -\frac{1}{2\sigma^2} s(\mathbf{a^\star}) \right]  \\
                   & \quad \times \int exp \left[ -\frac{1}{2}(\mathbf{a-a^\star})^T\frac{(\mathbf{H^TE^{-1}H}+\alpha^2\mathbf{G})}{\sigma^2}(\mathbf{a-a^\star}) \right] ~d(\mathbf{a-a^\star}) \\
                   & = (2\pi\sigma^2)^{-(N+P-M)/2}(\alpha^2)^{p/2}\|\mathbf{E}\|^{-1/2}\|\Lambda_p\|^{1/2} \| \mathbf{H^TE^{-1}H}+\alpha^2\mathbf{G} \|^{-1/2} \\
                   & \quad  \times exp\left[ -\frac{1}{2\sigma^2} s(\mathbf{a^\star}) \right]  \\
\end{aligned}
\end{equation}
</script>

then
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\frac{\partial L(\sigma^2,\alpha^2)}{\partial \sigma^2 } = 0  & \iff \frac{\partial \left( (\sigma^2)^{-(N+P-M)/2} exp \left[ \frac{s(\mathbf{a^\star})/2}{\sigma^2} \right]\right)}{\partial \sigma^2} \\
& \Rightarrow \sigma^2=\frac{s(\mathbf(a^\star))}{N+P-M}\\
\frac{\partial L(\sigma^2,\alpha^2)}{\partial \alpha^2 } = 0  & \quad \text{none-linear, resort to ABIC}
\end{aligned}
\end{equation}
</script>

substitute equation(12) into ABIC then rewrite.
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
ABIC(\alpha^2)= &-2log\left(\left( \frac{ 2\pi }{N+P-M}\color{red}{s(\mathbf{a^\star})} \right)^{\color{red}{-(N+P-M)/2}}\color{red}{(\alpha^2)^{p/2}} \right) \\
                &-2log\left( \|\mathbf{E}\|^{-1/2}\|\Lambda_p\|^{1/2} \color{red}{\| \mathbf{H^TE^{-1}H}+\alpha^2\mathbf{G} \|^{-1/2}} \right)\\
                &-2log\left( exp\left[ -\frac{1}{2} (N+P-M) \right] \right) \\
              = &(N+P-M) \log s(\mathbf{a^\star})-P \log \alpha^2 \\
                &+\log \| \mathbf{H^TE^{-1}H}+\alpha^2\mathbf{G} \|  + C
\end{aligned}
\end{equation}
</script>

**[Matrix Calculus](http://en.wikipedia.org/wiki/Matrix_calculus)**

Condition  | Expression    | Numerator layout | Denominator layout
---------- | ------------- | ---------------- | ------------------
**A** is not a<br/>function of **x**                        | $\frac{\partial \mathbf{x}^{\rm T}\mathbf{A}\mathbf{x}}{\partial \mathbf{x}}=$  | $\mathbf{x}^{\rm T}(\mathbf{A} + \mathbf{A}^{\rm T})$ | $(\mathbf{A} + \mathbf{A}^{\rm T})\mathbf{x}$
**A** is not a<br/>function of **x**<br/>**A** is symmetric | $\frac{\partial \mathbf{x}^{\rm T}\mathbf{A}\mathbf{x}}{\partial \mathbf{x}} =$ | $2\mathbf{x}^{\rm T}\mathbf{A}$                       | $2\mathbf{A}\mathbf{x}$

<br/>


to keep **prior** consistent with **likelihood**, hereby scale **G**, just give a qualitative analysis. 
<script type="math/tex; mode=display">
\begin{aligned}
& \large \mathbb{E} 
\left[ \left( \mathbf{d-Ha}\right)^T \frac{\mathbf{E}^{-1}}{\sigma^2} \left( \mathbf{d-Ha} \right) \right] = N,suppose\,\sigma^2=1 \\
& \large \mathbb{E}
\left(  \mathbf{a}^T \frac{\mathbf{G}^{\prime}}{\rho^2} \mathbf{a} \right)\, to\, be\, P\, with\, \rho^2=1  \\
& \large 
\Rightarrow  \mathbf{G}^{\prime} = \mathbf{G} * \frac{P}{ \bar{r} } \,with\,  \mathbf{ \bar{a} }^T \mathbf{G}  \mathbf{ \bar{a} } = \bar{r} \, and \,\, \bar{a}\,initial\,evaluation \end{aligned}
</script>
--------------

## other notes and notions.

Consider a vector-valued function $\mathbb{R}^n\mapsto\mathbb{R}^m$  of order $m \times 1$, such that 

<script type="math/tex; mode=display">\mathbf{F}(x)=\left[f_1(x),\cdots,f_m(x)\right]^T</script>

The diferentiation of such a vector-valued function $\mathbf{F}(\mathbf{x})$ by another vector **x** of order $n\times1$ is ambiguous
in the sense that the derivative 

<script type="math/tex; mode=display">\frac{\partial \mathbf{F(x)}}{\partial \mathbf{x}}</script>

can either be expressed as an $m \times n$ matrix ( **numerator** layout convention ), or as an $n \times m$ matrix ( **denominator** layout convention ), such that we have

<script type="math/tex; mode=display">
\frac{\partial \mathbf F (\mathbf X)} {\partial \mathbf{X}}=
\begin{bmatrix}
 \frac{\partial f_1(x)}{\partial x_1 } & \cdots & \frac{\partial f_1(x)}{\partial x_n}\\
 \vdots & \ddots & \vdots\\
 \frac{\partial f_m(x)}{\partial x_1 } & \cdots & \frac{\partial f_m(x)}{\partial x_n}\\
\end{bmatrix}
,\quad or \quad
\frac{\partial \mathbf F (\mathbf X)} {\partial \mathbf{X}}=
\begin{bmatrix}
 \frac{\partial f_1(x)}{\partial x_1 } & \cdots & \frac{\partial f_m(x)}{\partial x_1}\\
 \vdots & \ddots & \vdots\\
 \frac{\partial f_1(x)}{\partial x_n } & \cdots & \frac{\partial f_m(x)}{\partial x_n}\\
\end{bmatrix}
</script>




{% comment %}
---shell script to plot figure in this article-----
#!/usr/bin/sh
psbasemap -R-2/2/-12/6 -JX6i/1i -Bf1S    -Y4i  -K -P           --BASEMAP_TYPE=graph > a.ps
psbasemap -R           -JX1i/3i -Bf3W      -Y-2i -X3i -K -P -O  --BASEMAP_TYPE=graph >> a.ps
psbasemap -R0/2/-12/6  -JX3i/3i -Bg0.1/g1W -K -P -O           --BASEMAP_TYPE=plain >> a.ps
psbasemap -R-2/0/-12/6 -JX3i/3i -Bg0.1/g1E -K -P -O -X-3i     --BASEMAP_TYPE=plain >> a.ps

gawk 'BEGIN {   s_2=-2;
                s_1=-1;
		s0 = 0;
		s1 = 1;
		s2 = 2;
		ds = 1;
		dds=0.02;
                for(s=s_2;s<=s2;s=s+dds) { 
                    if ( s < s_1) {
			    printf "%6.2f %12.8f\n", s, (s-s_2)**3 ;
		    } else if ( s < s0 ){
		    	    printf "%6.2f %12.8f\n", s, 3*(s0-s)**3-6*ds*(s0-s)**2+4*ds**3;
	            } else if ( s < s1 ){
		            printf "%6.2f %12.8f\n", s, 3*(s-s0)**3-6*ds*(s-s0)**2+4*ds**3; 
	            } else if ( s < s2 ){
		            printf "%6.2f %12.8f\n", s, (s2-s)**3 ;
	            }
		} 
	    }'  | psxy -R-2/2/-12/6 -JX6i/3i -P -W2px/255/0/0 -K -O  >> a.ps

gawk 'BEGIN {   s_2=-2;
                s_1=-1;
		s0 = 0;
		s1 = 1;
		s2 = 2;
		ds = 1;
		dds=0.02;
                for(s=s_2;s<=s2;s=s+dds) { 
                    if ( s < s_1) {
			    printf "%6.2f %12.8f\n", s,  3*(s-s_2)**2 ;
		    } else if ( s < s0 ){
		    	    printf "%6.2f %12.8f\n", s, -9*(s0-s)**2+12*ds*(s0-s);
	            } else if ( s < s1 ){
		            printf "%6.2f %12.8f\n", s,  9*(s-s0)**2-12*ds*(s-s0); 
	            } else if ( s < s2 ){
		            printf "%6.2f %12.8f\n", s, -3*(s2-s)**2 ;
	            }
		} 
	    }' | psxy -R-2/2/-12/6 -JX6i/3i -P -W2px/0/255/0  -K -O  >> a.ps

gawk 'BEGIN {   s_2=-2;
                s_1=-1;
		s0 = 0;
		s1 = 1;
		s2 = 2;
		ds = 1;
		dds=0.2;
                for(s=s_2;s<=s2;s=s+dds) { 
                    if ( s < s_1) {
			    printf "%6.2f %12.8f\n", s,  6*(s-s_2) ;
		    } else if ( s < s0 ){
		    	    printf "%6.2f %12.8f\n", s, 18*(s0-s)-12*ds ;
	            } else if ( s < s1 ){
		            printf "%6.2f %12.8f\n", s, 18*(s-s0)-12*ds ; 
	            } else if ( s < s2 ){
		            printf "%6.2f %12.8f\n", s,  6*(s2-s) ;
	            }
		} 
	    }' | psxy -R-2/2/-12/6 -JX6i/3i -P -W2px/0/0/255  -K -O >> a.ps

pstext -R -J -P -K -O -H1 -N << EOF >> a.ps
# x,y,size,angle,fontno,justify,text   [T|M|B][L|C|R] (top/middle/bottom/left/center/right).
-2 -1  10  0 0  BC  S 
-1 -1  10  0 0  BC  S
 0 -1  10  0 0  BC  S
 1 -1  10  0 0  BC  S
 2 -1  10  0 0  BC  S
-1.9 -1.2 8  0 0  BC  j-2 
-0.9 -1.2 8  0 0  BC  j-1
 0.1 -1.2 8  0 0  BC  j
 1.1 -1.2 8  0 0  BC  j+1
 2.1 -1.2 8  0 0  BC  j+2
EOF
{% endcomment %}

